# -*- coding: utf-8 -*-
"""NLP 243 Homework 2 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hHpBTLDHtrHr_pMjBXhN8La2riLrovL
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import f1_score
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
import nltk
from nltk.util import ngrams
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("train_file", type=str, help="Path to the training data")
parser.add_argument("test_file", type=str, help="Path to the test data")
parser.add_argument("output_file", type=str, help="Path to save the output predictions")
args = parser.parse_args()
train_filepath = args.train_file
test_filepath = args.test_file
output_filepath = args.output_file

class EmbeddingLayer:
    def __init__(self, embedding_type='glove', embedding_dim=100, ngram_range=(1,2)):
        self.embedding_type = embedding_type
        self.embedding_dim = embedding_dim
        self.ngram_range = ngram_range
        self.word_to_idx = {}
        self.embeddings = None
        self.vectorizer = None

    def load_glove(self, glove_path):
        embeddings_dict = {}
        with open(glove_path, 'r', encoding='utf-8') as f:
            for line in f:
                values = line.split()
                word = values[0]
                vector = np.asarray(values[1:], dtype='float32')
                embeddings_dict[word] = vector
        return embeddings_dict

    def fit(self, sentences):
        if self.embedding_type == 'glove':
            # Note: You'll need to provide the path to your GloVe embeddings
            glove_dict = self.load_glove('glove.6B.100d.txt')

            # Create vocabulary and embedding matrix
            word_to_idx = {'<PAD>': 0, '<UNK>': 1}
            embeddings_matrix = [np.zeros(self.embedding_dim), np.random.normal(scale=0.1, size=(self.embedding_dim,))]

            for sentence in sentences:
                for word in sentence:
                    if word not in word_to_idx and word in glove_dict:
                        word_to_idx[word] = len(word_to_idx)
                        embeddings_matrix.append(glove_dict[word])

            self.word_to_idx = word_to_idx
            self.embeddings = torch.FloatTensor(embeddings_matrix)

        elif self.embedding_type == 'countvectorizer':
            self.vectorizer = CountVectorizer(lowercase=True)
            # Join words to create documents for CountVectorizer
            docs = [' '.join(sentence) for sentence in sentences]
            self.vectorizer.fit(docs)
            self.word_to_idx = {word: idx + 2 for idx, word in enumerate(self.vectorizer.vocabulary_)}
            self.word_to_idx['<PAD>'] = 0
            self.word_to_idx['<UNK>'] = 1

        elif self.embedding_type == 'ngram':
            ngram_vocab = set()
            for sentence in sentences:
                for n in range(self.ngram_range[0], self.ngram_range[1] + 1):
                    sentence_ngrams = ngrams(sentence, n)
                    ngram_vocab.update(' '.join(gram) for gram in sentence_ngrams)

            self.word_to_idx = {ngram: idx + 2 for idx, ngram in enumerate(ngram_vocab)}
            self.word_to_idx['<PAD>'] = 0
            self.word_to_idx['<UNK>'] = 1

class SlotTaggingDataset(Dataset):
    def __init__(self, utterances, tags, embedding_layer, tag_vocab):
        self.utterances = utterances
        self.tags = tags
        self.embedding_layer = embedding_layer
        self.tag_vocab = tag_vocab

    def __len__(self):
        return len(self.utterances)

    def __getitem__(self, idx):
        utterance = self.utterances[idx]
        tag = self.tags[idx]

        # Convert words to indices based on embedding type
        if self.embedding_layer.embedding_type == 'ngram':
            # Create n-grams for the utterance
            input_indices = []
            for n in range(self.embedding_layer.ngram_range[0], self.embedding_layer.ngram_range[1] + 1):
                if len(utterance) >= n:
                    utterance_ngrams = ngrams(utterance, n)
                    for gram in utterance_ngrams:
                        gram_str = ' '.join(gram)
                        input_indices.append(self.embedding_layer.word_to_idx.get(gram_str, 1))  # 1 is <UNK>
        else:
            input_indices = [self.embedding_layer.word_to_idx.get(word, 1) for word in utterance]

        target_indices = [self.tag_vocab.get(t, 0) for t in tag]

        return torch.tensor(input_indices, dtype=torch.long), torch.tensor(target_indices, dtype=torch.long)

class BiLSTMSlotTagger(nn.Module):
    def __init__(self, embedding_layer, hidden_dim, output_dim, num_layers=2, dropout=0.4):
        super(BiLSTMSlotTagger, self).__init__()

        if embedding_layer.embedding_type == 'glove':
            self.embedding = nn.Embedding.from_pretrained(embedding_layer.embeddings, padding_idx=0)
            input_dim = embedding_layer.embedding_dim
        else:
            vocab_size = len(embedding_layer.word_to_idx)
            input_dim = embedding_layer.embedding_dim
            self.embedding = nn.Embedding(vocab_size, input_dim, padding_idx=0)

        self.bilstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional

    def forward(self, x):
        embedded = self.dropout(self.embedding(x))
        lstm_out, _ = self.bilstm(embedded)
        lstm_out = self.dropout(lstm_out)
        output = self.fc(lstm_out)
        return output

def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=5):
    best_f1 = 0

    for epoch in range(num_epochs):
        # Training
        model.train()
        total_loss = 0
        for batch_idx, (input_tensor, target_tensor) in enumerate(train_loader):
            input_tensor, target_tensor = input_tensor.to(device), target_tensor.to(device)

            optimizer.zero_grad()
            output = model(input_tensor)

            output = output.view(-1, output.shape[-1])
            target_tensor = target_tensor.view(-1)

            loss = criterion(output, target_tensor)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

        # Evaluation
        model.eval()
        all_preds = []
        all_true = []

        with torch.no_grad():
            for input_tensor, target_tensor in test_loader:
                input_tensor = input_tensor.to(device)
                target_tensor = target_tensor.to(device)

                outputs = model(input_tensor)
                preds = torch.argmax(outputs, dim=-1)

                # Remove padding tokens
                mask = target_tensor != 0
                preds = preds[mask]
                true = target_tensor[mask]

                all_preds.extend(preds.cpu().numpy())
                all_true.extend(true.cpu().numpy())

        f1 = f1_score(all_true, all_preds, average='weighted')
        print(f"Epoch {epoch+1} - F1 Score: {f1:.4f}")

        if f1 > best_f1:
            best_f1 = f1
            # Save best model
            torch.save(model.state_dict(), 'best_model.pt')

    return best_f1

def predict_and_save(model, test_loader, test_df, tag_vocab, device, output_path='slot_tagging_predictions.csv'):
    """
    Generate predictions and save them in the required CSV format
    """
    model.eval()
    # Create reverse vocabulary for converting indices back to tags
    idx_to_tag = {idx: tag for tag, idx in tag_vocab.items()}

    all_predictions = []
    current_sequence = []

    with torch.no_grad():
        for batch in test_loader:
            input_tensor = batch[0].to(device)
            sequence_lengths = (input_tensor != 0).sum(dim=1)

            outputs = model(input_tensor)
            predictions = torch.argmax(outputs, dim=-1)

            # Convert predictions to tags and handle padding
            for pred_seq, length in zip(predictions, sequence_lengths):
                # Only take the valid (non-padding) predictions
                valid_preds = pred_seq[:length].cpu().numpy()
                # Convert indices to tags
                pred_tags = [idx_to_tag[idx] for idx in valid_preds]
                all_predictions.append(pred_tags)

    # Create submission dataframe
    submission_df = pd.DataFrame({
        'ID': test_df['ID'],
        'IOB Slot tags': [' '.join(pred_seq) for pred_seq in all_predictions]
    })

    # Save predictions
    submission_df.to_csv(output_path, index=False)
    print(f"Predictions saved to {output_path}")

    return submission_df

def main():
    # Load your data
    train_df = pd.read_csv(train_filepath)
    test_df = pd.read_csv(test_filepath)

    train_utterances = train_df['utterances'].apply(lambda x: x.split()).tolist()
    test_utterances = test_df['utterances'].apply(lambda x: x.split()).tolist()
    train_tags = [tags.split() for tags in train_df['IOB Slot tags'].tolist()]

    # Create tag vocabulary
    tag_vocab = {tag: idx for idx, tag in enumerate(set([tag for tag_seq in train_tags for tag in tag_seq]))}

    # Initialize embedding layer (choose one: 'glove', 'countvectorizer', or 'ngram')
    embedding_layer = EmbeddingLayer(embedding_type='glove', embedding_dim=100)
    embedding_layer.fit(train_utterances)

    # Create datasets
    train_dataset = SlotTaggingDataset(train_utterances, train_tags, embedding_layer, tag_vocab)
    test_tags = [['O'] * len(utterance) for utterance in test_utterances]
    test_dataset = SlotTaggingDataset(test_utterances, test_tags, embedding_layer, tag_vocab)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,
                          collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0),
                                                    pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)))
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,
                         collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0),
                                                   pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)))

    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = BiLSTMSlotTagger(
        embedding_layer=embedding_layer,
        hidden_dim=512,
        output_dim=len(tag_vocab),
        num_layers=3,
        dropout=0.4
    ).to(device)

    # Initialize training components
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Train and evaluate
    best_f1 = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, device, num_epochs=5)
    print(f"Best F1 Score: {best_f1:.4f}")

    # Load best model for predictions
    model.load_state_dict(torch.load('best_model.pt'))

    # Generate and save predictions
    predictions_df = predict_and_save(
        model=model,
        test_loader=test_loader,
        test_df=test_df,
        tag_vocab=tag_vocab,
        device=device,
        output_path='submission.csv'
    )

    # Print first few predictions as a sample
    print("\nSample predictions:")
    print(predictions_df.head())

if __name__ == "__main__":
    main()